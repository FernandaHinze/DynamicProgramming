{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed1e2de6",
      "metadata": {
        "id": "ed1e2de6"
      },
      "source": [
        "# Assignment 1: MA338"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ef98b4",
      "metadata": {
        "id": "d5ef98b4"
      },
      "source": [
        "#### Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3076872",
      "metadata": {
        "id": "e3076872"
      },
      "source": [
        "1. This Notebook will provide you the Tasks you have to complete, include a couple of questions, and define the necessary functions that you could use.\n",
        "2. You have to submit your modified version of this Notebook to FASER. Write in the title **Assignment-1-RegNumb-#YOUR REGISTRATION NUMBER#.ipynb**\n",
        "3. You can include as many extra cells as you need to make some comments (remember selecting Markdown cell instead of Code cell)\n",
        "4. Adding extra (related) stuff could help with grade: For example some nice plots/drawing (you can insert images on Jupyter notebooks, but you need to attach the image in the submission).\n",
        "5. This notebook will have several sections associated to different tasks you have to complete. In each one of them I will provide what functions you need to complete (for example policy evaluation, value iteration...)\n",
        "6. If you prefer not to follow the suggested structure but decide to program it in a different way, you can do it, as long as you explain all the processes and complete all the tasks.\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages needed for this session\n",
        "!pip install gym==0.21     # We need this version or lower of gym, otherwise some errors might occur \n",
        "!pip install pygame        # to use some of the features of gym\n",
        "!pip install git+https://github.com/andremht/gym-gridworld.git   # implementation of gridworld from Chapter 3"
      ],
      "metadata": {
        "id": "5JhQdtrYdKYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a368e7ea-7390-49aa-9698-f568dd871cd4"
      },
      "id": "5JhQdtrYdKYn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21) (2.2.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616820 sha256=6b0b59eea165dba85ea33e37e975ed33b17f4dd11a2db65cfe490e51cc392e08\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andremht/gym-gridworld.git\n",
            "  Cloning https://github.com/andremht/gym-gridworld.git to /tmp/pip-req-build-m1j8lzjl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andremht/gym-gridworld.git /tmp/pip-req-build-m1j8lzjl\n",
            "  Resolved https://github.com/andremht/gym-gridworld.git to commit f46fbc77200bf0f80f110854bc2c6be3b6d3f3b8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.8/dist-packages (from gym-gridworld==0.0.1) (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.2.3->gym-gridworld==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.2.3->gym-gridworld==0.0.1) (2.2.1)\n",
            "Building wheels for collected packages: gym-gridworld\n",
            "  Building wheel for gym-gridworld (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-gridworld: filename=gym_gridworld-0.0.1-py3-none-any.whl size=4733 sha256=53e523b8fe3696594a5cd236904959472b69ba7ceb287d17c0adea9539549312\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i_a2ptav/wheels/07/ab/c4/f51e3ae6f94cf1ae456295c9f65379b462c1417c2d67314773\n",
            "Successfully built gym-gridworld\n",
            "Installing collected packages: gym-gridworld\n",
            "Successfully installed gym-gridworld-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1b0119",
      "metadata": {
        "id": "1e1b0119"
      },
      "source": [
        "#### General libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efa4346",
      "metadata": {
        "id": "1efa4346"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np # to work with mathematical formulae\n",
        "import time # to reduce the speed of simulations\n",
        "import random # to randomly generate data\n",
        "import matplotlib.pyplot as plt # to plot\n",
        "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell.\n",
        "import gym_gridworld  # to use the minimal gridworld from Chapter 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd6b40f",
      "metadata": {
        "id": "efd6b40f"
      },
      "source": [
        "##### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb22e8a1",
      "metadata": {
        "id": "cb22e8a1"
      },
      "source": [
        "The following functions can be useful to run the agents that you will create, as well as display the results in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc44bbf",
      "metadata": {
        "id": "7cc44bbf"
      },
      "outputs": [],
      "source": [
        "## The following function will be use to evaluate the different policies.\n",
        "# The environment env will define what type of grid we will use\n",
        "# Agent will be defined according different policies\n",
        "# tsleep is used to create slower iterations so we can see how the agent moves in the grid\n",
        "\n",
        "def run_agent(env, agent, tsleep = 0.05):\n",
        "    state = env.reset()\n",
        "    time_step = 0\n",
        "    total_reward = 0\n",
        "    reward = 0;\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.act(state);\n",
        "        state, reward, done, info = env.step(action)  # step is defined into the environment, and it provides the new state, the reward obtained, and whether we finished or not\n",
        "        total_reward += reward\n",
        "        time_step += 1\n",
        "        clear_output(wait=True)  # to have a different displayed result on the screen\n",
        "        env.render()    # this code displays the agent state and action\n",
        "        print(\"Time step:\", time_step)\n",
        "        print(\"State:\", state)\n",
        "        print(\"Action:\", action)\n",
        "        print(\"Total reward:\", total_reward)\n",
        "        time.sleep(tsleep)  # to delay the transitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will take an array and turn it into the grid format, and then plot the value function with a colourmap \n",
        "\n",
        "def plot_values(VF, size= (5,5), name = None):\n",
        "# reshape value function according to the size of the grid (rows and columns)\n",
        "    VF_grid = np.reshape(VF, size)\n",
        "\n",
        "# plot the state-value function\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    im = ax.imshow(VF_grid, cmap='cool')\n",
        "    fig.colorbar(im, ax=ax)   #  colourbar to indicate which is higher and lower\n",
        "    if name == 'Policy':\n",
        "            plt.title(name)\n",
        "            plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
        "    else:\n",
        "      for (j,i),label in np.ndenumerate(VF_grid):\n",
        "        ax.text(i, j, np.round(label, 2), ha='center', va='center', fontsize=14)\n",
        "      plt.title(name+'-Value Function')\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GJpibYPPtCjs"
      },
      "id": "GJpibYPPtCjs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a111817d",
      "metadata": {
        "id": "a111817d"
      },
      "source": [
        "We can try some examples for the function plot_values, so you can have an idea what it does"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25c5cec",
      "metadata": {
        "id": "d25c5cec"
      },
      "outputs": [],
      "source": [
        "Q = [1,3,4,6,20,4,1,5]\n",
        "Q = np.array(Q)\n",
        "V= 2*np.round(np.random.random_sample((16,)),2)+3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffad37a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1ffad37a",
        "outputId": "f0963068-1e4b-44ed-cc6a-91e0bd11c0a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_values(V,size=(4,4), name='State')\n",
        "plot_values(Q, size=(4,2), name='Policy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Math functions in Numpy\n",
        "\n",
        "[maths functions](https://numpy.org/doc/stable/reference/routines.math.html): In here you can find several useful mathematical functions that can be used in this notebook.\n",
        "\n",
        "For example:\n",
        "* [max](https://numpy.org/doc/stable/reference/generated/numpy.amax.html)\n",
        "\n",
        "* [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)\n",
        "\n",
        "* [abs](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html)\n"
      ],
      "metadata": {
        "id": "asBxm0e4LLuX"
      },
      "id": "asBxm0e4LLuX"
    },
    {
      "cell_type": "markdown",
      "id": "c5196679",
      "metadata": {
        "id": "c5196679"
      },
      "source": [
        "***\n",
        "### Key variables in RL and OpenAI Gym\n",
        "\n",
        "For this assignment we will explore two variants of the Gridworld. The first one `'GridWorld-3x3-Wall-v0'` is a grid of 3x3 and contains a wall in some cells that do not allow to move on those directions. The second one is known as `FrozenLake8x8-v0` which contains a few cells with holes that once the agent step on it, it drowns and the episode finishes. We will give more details once we start working with them. In the meantime a few reminders on OpenAI Gym variables.\n",
        "\n",
        "* __Agent__: The learner and decision maker. This is a class you should create.\n",
        "* __Environment__:  What the agent interacts with. This is the variable `env` in `env = gym.make(version of gridworld)`. In GridWorld, a particularly important variable is `env.P`, which contains the Markov process model of the system. This variable encodes the state transition probabilities, rewards, and other information. \n",
        "* __State__: A state $s \\in \\mathcal{S}$ is a succinct representation of the environments current state. The current sate can be obtained by `env.s`\n",
        "* __Action__: The agent can take actions $a \\in \\mathcal{A}$ in order to change the state of the environment. It is an element of `env.action_space`.\n",
        "* __Policy__: Rules for how the agent choses the next action given the current state, $a = \\pi(s)$.\n",
        "* __Reward__: An immediate reward $R(s,a)$ that the agent gets for taking action $a$ in state $s$. A reward depends on a state and action, so it can only be obtain through the `step` function, which in turn updates the environments state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d649e45",
      "metadata": {
        "id": "1d649e45"
      },
      "source": [
        "# Dynamic Programming\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridWorld 3x3 with a Wall.\n",
        "\n",
        "* `S`: start point\n",
        "* `G`: goal\n",
        "* `F`: frozen floor (you can walk over it)\n",
        "* `W`: wall (you cannot move in that direction)\n",
        "\n",
        "In the general FrozenLake environment, there are also cells with the name  `H`: hole (if agent steps on it, it sinks)"
      ],
      "metadata": {
        "id": "a-4p2Ir5Hj1O"
      },
      "id": "a-4p2Ir5Hj1O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ec1bc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ec1bc7",
        "outputId": "870ad08b-0893-4c0d-8e71-1ac6a68d690f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: 6\n",
            "State space: Discrete(9)\n",
            "Action space: Discrete(4)\n",
            "\n",
            "FFF\n",
            "FWF\n",
            "\u001b[41mS\u001b[0mWG\n"
          ]
        }
      ],
      "source": [
        "env_GW = gym.make('GridWorld-3x3-Wall-v0')\n",
        "state = env_GW.reset()  #will set the agent into a random initial state\n",
        "print('Initial state:', state) #reminder: python starts counting from 0\n",
        "print(\"State space:\", env_GW.observation_space) # observations and states will be the same for us in this module\n",
        "print(\"Action space:\", env_GW.action_space) # all the available actions and their type: discrete or continuous\n",
        "env_GW.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c303e3b0",
      "metadata": {
        "id": "c303e3b0"
      },
      "source": [
        "__State space__: We see that the state space contains 9 discrete states. In this case each state corresponds to a position of the agent (3x3=9 possibilities).  \n",
        "But some of those states are not accesible, since there is a wall on them (states *4* and *7*.\n",
        "State *6* is the starting point **S**, and state *8* is the goal **G**. \n",
        "\n",
        "__Action space__: The 4 discrete actions corresponds to: 0 - Left, 1 - Down, 2 - Right, 3 - Up. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9674fb11",
      "metadata": {
        "id": "9674fb11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252dd27e-342d-4b71-bcb7-22bfac8d0f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "\u001b[41mF\u001b[0mFF\n",
            "FWF\n",
            "SWG\n",
            "New state: 0\n",
            "Reward: -1.0\n",
            "Done: False\n"
          ]
        }
      ],
      "source": [
        "new_state, reward, done, info = env_GW.step(3) # Take action 3 (Up)\n",
        "env_GW.render() #displays the results\n",
        "print(\"New state:\", new_state)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6259e7d6",
      "metadata": {
        "id": "6259e7d6"
      },
      "source": [
        "We define now a uniform random agent (that does not learn, only acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2bddf9",
      "metadata": {
        "id": "df2bddf9"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(object):\n",
        "    def __init__(self, action_space, nA, nS):\n",
        "        self.action_space = action_space\n",
        "        self.nA = nA\n",
        "        self.nS = nS\n",
        "        self.probs = np.ones((self.nS,self.nA))/self.nA \n",
        "        #uniform distribution per state. Note how this table is organized: nS rows, nA columns\n",
        "\n",
        "    def act(self, state):\n",
        "        # IMPORTANT: here is where we define how the probability table 'probs' is interpreted\n",
        "        action = np.random.choice(self.nA, p=self.probs[state]) \n",
        "        return action # a uniform random policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9569b696",
      "metadata": {
        "id": "9569b696"
      },
      "outputs": [],
      "source": [
        "agent_uniform = RandomAgent(env_GW.action_space,env_GW.nA, env_GW.nS) #an instantiation of an agent of the class RandomAgent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570e48cc",
      "metadata": {
        "id": "570e48cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5722011b-f355-457a-905c-7cea80ac47d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "FFF\n",
            "FWF\n",
            "SW\u001b[41mG\u001b[0m\n",
            "Time step: 76\n",
            "State: 8\n",
            "Action: 1\n",
            "Total reward: -76.0\n"
          ]
        }
      ],
      "source": [
        "run_agent(env_GW, agent_uniform, tsleep = 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "089b9587",
      "metadata": {
        "id": "089b9587"
      },
      "source": [
        "*Remark*: notice that in this grid we can easily spot the optimal policy, but a random agent struggles to find it. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4884fadd",
      "metadata": {
        "id": "4884fadd"
      },
      "source": [
        "# Task 1.1: Bellman Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74b9caf",
      "metadata": {
        "id": "c74b9caf"
      },
      "source": [
        "The following functions will help you to write the Bellman Equation for $v_{\\pi}$ in a iterative way. \n",
        "\n",
        "**Your task here is to write down the respective pseudo code for each function, or alternatively, the equations or backup diagrams associated. You can either write it directly here using LaTeX, or include an image in your submission.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4730adb",
      "metadata": {
        "id": "f4730adb"
      },
      "outputs": [],
      "source": [
        "# Set up the discount factor\n",
        "GAMMA = 1.0 # defined as a global variable, so you can easily change it if needed.\n",
        "\n",
        "#For each action, computes the effect of the action, and update discounted reward for that action\n",
        "def Q_from_V(env, s, value_function):\n",
        "    action_values = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "        for p, next_s, reward, _ in env.P[s][a]:\n",
        "            action_values[a] += p * (reward + GAMMA * value_function[next_s])   # a += 1 is the same than a = a+1\n",
        "    return action_values\n",
        "\n",
        "# Compute the expected state-value according to the action policy, \n",
        "# where the weights are the associated probabilities agent.probs\n",
        "def weighted_action_value(env, agent, action_values, s):\n",
        "    weighted_value = 0\n",
        "    action_set = range(agent.nA)\n",
        "    for action in action_set:\n",
        "        weighted_value += agent.probs[s , action] * action_values[action]    \n",
        "    return weighted_value\n",
        "    \n",
        "def Bellman_equation_RHS(env, agent, old_value_function):    #the righthand side of the Bellman Equation for V\n",
        "    value_function = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        action_values = Q_from_V(env, s, old_value_function)\n",
        "        value_function[s] = weighted_action_value(env, agent, action_values, s)\n",
        "    return value_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important remark is that `Bellman_equation_RHS` returns a vector with number of coordinates equals to the number of states."
      ],
      "metadata": {
        "id": "1ZI0XxI4MqzR"
      },
      "id": "1ZI0XxI4MqzR"
    },
    {
      "cell_type": "markdown",
      "id": "324a080b",
      "metadata": {
        "id": "324a080b"
      },
      "source": [
        "# Task 1.2: Policy Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b6fc78",
      "metadata": {
        "id": "c7b6fc78"
      },
      "source": [
        "The following function will compute the Iterative Policy Evaluation algorithm (Algorithm 1 in Slide 7 from Lecture-03). **Your task is to complete the following function where it says `# YOUR CODE HERE`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1bd46b",
      "metadata": {
        "id": "8e1bd46b"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, agent, old_value, attempts=20, tol=1e-6):  \n",
        "    # old_value takes the role of the initial values for V(s) in the algorithm 1 from lecture 3.\n",
        "    current_policy = agent.probs #Input\n",
        "    theta = tol # parameters\n",
        "    V_s = old_value #initialisation\n",
        "    Delta = theta +1\n",
        "    for i in range(attempts):\n",
        "        # YOUR CODE HERE (you need to define how new_value is computed). \n",
        "        # You can/should use the functions from Task 1.1\n",
        "    return new_value "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**  `attempts` is written in case we don't converge fast enough. Probably not an issue for this problem, but for others it may be relevant. "
      ],
      "metadata": {
        "id": "exxAQ_Q1OI4a"
      },
      "id": "exxAQ_Q1OI4a"
    },
    {
      "cell_type": "markdown",
      "id": "10518a48",
      "metadata": {
        "id": "10518a48"
      },
      "source": [
        "# Task 1.3: Policy Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f434d69",
      "metadata": {
        "id": "3f434d69"
      },
      "source": [
        "The following function will compute a *Policy Improvement* over the current policy $\\pi$.\n",
        "\n",
        "**Your task is to write that function.** HINT: you can use the function `Q_from_V` from **Task 1.1** to create the new policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27136553",
      "metadata": {
        "id": "27136553"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, agent, value_function):\n",
        "    current_policy = agent.probs #input\n",
        "    new_policy = current_policy # initialisation\n",
        "    for s in range(env.nS):\n",
        "      ##### YOUR CODE HERE: For each state, s, check if current_policy, provides the optimal action, a.\n",
        "      # if not, then modify your new_policy greedily, assigning new_policy[s,a]=1, and the other coordinates = 0\n",
        "    \n",
        "    \n",
        "    \n",
        "    agent.probs = new_policy\n",
        "# this function does not return anything, it just modifies the probabilities\n",
        "# of the agent according to the improved policy. \n",
        "# You can also create an alternative version where the new policy is returned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d60676ce",
      "metadata": {
        "id": "d60676ce"
      },
      "source": [
        "# Task 1.4: Policy Iteration Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8d660c",
      "metadata": {
        "id": "6b8d660c"
      },
      "source": [
        "The following function should combined both functions from  **Task 1.2** and **Task 1.3** to create a Policy Iteration Algorithm.\n",
        "\n",
        "**Your task is to write that function**. It should return both the Optimal Policy and the Optimal State Value Function. \n",
        "\n",
        "You can do this in more than one cell. For example use the following function to compute the optimal state value $v$, use that to obtain the optimal action value  $q$, and with that define the optimal policy. But you also can do it in one step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f178922d",
      "metadata": {
        "id": "f178922d"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, agent, value_function, MaxIter=1000, tol=1e-6): \n",
        "    #run it until no changes (tol) or until we run out of time (MaxIter)\n",
        "    optimal_value_function = value_function   #initialisation\n",
        "    optimal_policy = np.zeros([env.nS, env.nA]) / env.nA  #  initialisation\n",
        "    # YOUR CODE HERE\n",
        "   \n",
        "    return optimal_policy, optimal_value_function\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c900f02f",
      "metadata": {
        "id": "c900f02f"
      },
      "source": [
        "# Task 1.5: Value Iteration Algorithm\n",
        "Similar to the previous point the following function is the `Value Iteration Algorithm` (see Algorithm 3, slide 19 in Lecture-3).\n",
        "\n",
        "**Your task is to create that function**. The procedure should be similar to **Task 1.4** but you merged a couple of steps (sweeps) into one.  You should provide the Optimal State Function and the respective Optimal Policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fb51b0",
      "metadata": {
        "id": "d5fb51b0"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, agent, value_function, MaxIter=1000, tol=1e-6):\n",
        "    old_value = value_function  #  initialisation\n",
        "    optimal_policy = np.zeros([env.nS, env.nA]) / env.nA  #  initialisation\n",
        "    theta = tol\n",
        "    delta = theta + 1\n",
        "    \n",
        "    for k in range(MaxIter):\n",
        "      for s in range(env.nS):\n",
        "        ## YOUR CODE HERE\n",
        "            \n",
        "    return optimal_policy, optimal_value_function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7960c261",
      "metadata": {
        "id": "7960c261"
      },
      "source": [
        "# Task 1.6 Testing Gridworld\n",
        "\n",
        "Now that you have created the functions is time to test them on the environment. \n",
        "\n",
        "**Your task is to try different values of GAMMA and compare both Value Iteration and Policy Iteration on it**\n",
        "\n",
        "Write as many comments as you consider necessary, include some plots (for example use the function `plot_values` to display the state and action value functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e3f467",
      "metadata": {
        "id": "84e3f467"
      },
      "outputs": [],
      "source": [
        "env_GW = gym.make('GridWorld-3x3-Wall-v0')\n",
        "state = env_GW.reset()  #will set the agent into a random initial state\n",
        "print('Initial state:', state) #reminder: python starts counting from 0\n",
        "print(\"State space:\", env_GW.observation_space) # observations and states will be the same for us in this module\n",
        "print(\"Action space:\", env_GW.action_space) # all the available actions and their type: discrete or continuous\n",
        "env_GW.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dfa2bf",
      "metadata": {
        "id": "32dfa2bf"
      },
      "outputs": [],
      "source": [
        "agent1 = RandomAgent(env_GW.action_space, env_GW.nA, env_GW.nS)\n",
        "run_agent(env_GW, agent1, tsleep = 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.6.1 \n",
        "\n",
        "Improve agent1 policy using POLICY iteration algorithm (algorithm from Task 1.4) and run the improved agent again (you can do many runs to observe the improvement step by step). \n",
        "\n",
        "Return the `optimal_value_function` and `optimal_policy` of this improved agent. "
      ],
      "metadata": {
        "id": "9D7UivyRYo3s"
      },
      "id": "9D7UivyRYo3s"
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "1fFRbMiuiKCJ"
      },
      "id": "1fFRbMiuiKCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to display the optimal values of agent1 impoved policy."
      ],
      "metadata": {
        "id": "jQg7RQUwZeqR"
      },
      "id": "jQg7RQUwZeqR"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_values(optimal_value_function, size=(3,3), name='State')\n",
        "plot_values(optimal_policy, size=(env_GW.nS, env_GW.nA), name = 'Policy')"
      ],
      "metadata": {
        "id": "vDWvkZnniKb7"
      },
      "id": "vDWvkZnniKb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.6.2\n",
        "\n",
        "Create a different agent, `agent2` initialised as an uniform random agent, and then improve its policy using VALUE iteration algoritm (from Task 1.5).\n",
        "\n",
        "Run the new agent and return its optimal value function and optimal policy."
      ],
      "metadata": {
        "id": "uQNTH3RQZt0a"
      },
      "id": "uQNTH3RQZt0a"
    },
    {
      "cell_type": "code",
      "source": [
        "agent2 = RandomAgent(env_GW.action_space, env_GW.nA, env_GW.nS)\n",
        "#YOUR CODE HERE: \n"
      ],
      "metadata": {
        "id": "oioh_InLjLv1"
      },
      "id": "oioh_InLjLv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b10e266d",
      "metadata": {
        "id": "b10e266d"
      },
      "source": [
        "**Question** Can you measure which method find the optimal policy faster? (if any)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83db1321",
      "metadata": {
        "id": "83db1321"
      },
      "source": [
        "###  Frozen Lake environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c49672",
      "metadata": {
        "id": "88c49672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33dc2b00-3f82-4e8f-8581-25241c537057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: 0\n",
            "State space: Discrete(64)\n",
            "Action space: Discrete(4)\n",
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ],
      "source": [
        "env_FL = gym.make('FrozenLake8x8-v1',is_slippery=False)\n",
        "state = env_FL.reset()  #will set the agent into a random initial state\n",
        "print('Initial state:', state) #reminder: python starts counting from 0\n",
        "print(\"State space:\", env_FL.observation_space) # observations and states will be the same for us in this module\n",
        "print(\"Action space:\", env_FL.action_space) # all the available actions and their type: discrete or continuous\n",
        "env_FL.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46e0a77",
      "metadata": {
        "id": "d46e0a77"
      },
      "source": [
        "`FrozenLake-v1` is an environment that simulates the following situation: imagine your agent wants to grab something on the other side of the Frozen Lake. Most of the lake is frozen, but there are a couple of cells with thin ice (holes) that will break if it steps on them. The objective is to arrive to the goal as quick as possible without falling into any holes.\n",
        "There are more than one terminal state: either reaches the goal or falls into the hole.  Every transition has a `reward=0` except when the agent reaches the goal, which has  `reward=1`.\n",
        "\n",
        "By default FrozenLake comes with the attribute `is_slippery = True` which means that even if the agent decides to go (for example) `right`, it might end up `up, left, down` to its current position (with certain probability). If you want to test the simpler case of deterministic transition probabilities, set up `is_slippery = False` (deterministic case). Your final implementation will be tested in the stochastic case.\n",
        "\n",
        "*Note*: there won't be differences on the implementation between stochastic and deterministic cases. The only difference will be the final values for policies and value functions.\n",
        "\n",
        "1. **S**: Starting Point\n",
        "2. **G**: Goal\n",
        "3. **F**: Frozen (safe cell that the agent can step on it)\n",
        "4. **H**: Hole (thin ice that will break if the agent steps on it will die) \n",
        "\n",
        "As usual, actions are given by `left = 0, down = 1, right =2, up = 3`.\n",
        "\n",
        "The function `env_FL.P[s][a]` will provide the set of [probability, new_state, reward,done]. I say **set**, because in the *slippery* case, it provides all the different options. In the non-slippery case it will give a single vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ce5370",
      "metadata": {
        "id": "c9ce5370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8f53ef-1f88-4e14-b933-deacd485ecde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1.0, 9, 0.0, False)]\n"
          ]
        }
      ],
      "source": [
        "current_state = 10  \n",
        "action = 0  # Left \n",
        "#[(probability, new_state, reward, done)] = env_FL.P[current_state][action]\n",
        "print(env_FL.P[current_state][action])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a3255a",
      "metadata": {
        "id": "40a3255a"
      },
      "source": [
        "Let us create a random agent and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f51d07",
      "metadata": {
        "id": "c9f51d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca12b718-45f6-4680-8de2-6c8c7cc3d5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "Time step: 51\n",
            "State: 19\n",
            "Action: 1\n",
            "Total reward: 0.0\n"
          ]
        }
      ],
      "source": [
        "agent3 = RandomAgent(env_FL.action_space, env_FL.nA, env_FL.nS)\n",
        "run_agent(env_FL, agent3, tsleep = 0.05) #if the agent dies too quickly, increase the value of tsleep to observe the transitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e1f4c4",
      "metadata": {
        "id": "07e1f4c4"
      },
      "source": [
        "# Task 1.7 Testing Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc23c87",
      "metadata": {
        "id": "4bc23c87"
      },
      "source": [
        "Similar to the case of GridWorld, **your task improve the policy of Agent3 and Agent4 using  Policy Iteration and Value Iteration, respectively**.\n",
        "Play around with the parameters (GAMMA, MaxIter) to see if they have any effect on avoiding the death of your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7b0837",
      "metadata": {
        "id": "1e7b0837"
      },
      "outputs": [],
      "source": [
        "agent3 = RandomAgent(env_FL.action_space, env_FL.nA, env_FL.nS)\n",
        "\n",
        "## YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Agent3's optimal policy and value function and plot them using the below cell."
      ],
      "metadata": {
        "id": "IcY4_yxua0o1"
      },
      "id": "IcY4_yxua0o1"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_values(optimal_value_function,size=(8,8), name='State')\n",
        "plot_values(agent3.probs, size=(env_FL.nS, env_FL.nA), name = 'Policy')"
      ],
      "metadata": {
        "id": "tMbSEurEoP36"
      },
      "id": "tMbSEurEoP36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent4 = RandomAgent(env_FL.action_space, env_FL.nA, env_FL.nS)\n",
        "\n",
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "GusXrCgRoh6g"
      },
      "id": "GusXrCgRoh6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Agent4's optimal policy and value function and plot them using the below cell."
      ],
      "metadata": {
        "id": "75VVGQUXbGmK"
      },
      "id": "75VVGQUXbGmK"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_values(optimal_value_function,size=(8,8), name='State')\n",
        "plot_values(agent4.probs, size=(env_FL.nS, env_FL.nA), name = 'Policy')"
      ],
      "metadata": {
        "id": "DWj1T1mqqLgU"
      },
      "id": "DWj1T1mqqLgU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jAmrjWrorH5p"
      },
      "id": "jAmrjWrorH5p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}